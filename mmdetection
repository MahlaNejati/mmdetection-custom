from mmdet.apis import inference_detector, init_detector, show_result_pyplot
Sosm90!

_base_ = [
    '../_base_/models/mask_rcnn_r50_fpn.py',
    '../_base_/datasets/laboro_tomato_instance.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]




mmdet/datasets/laboro_tomato.py 



python tools/test.py configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py \
                     tomato/laboro_tomato_48ep.pth --show



from .coco import CocoDataset
from .builder import DATASETS


@DATASETS.register_module()
class LaboroTomato(CocoDataset):
    CLASSES = ('b_fully_ripened', 'b_half_ripened', 'b_green', 
               'l_fully_ripened', 'l_half_ripened', 'l_green')


dataset_type = 'LaboroTomato'





# Choose to use a config and initialize the detector
config = 'configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'
# Setup a checkpoint file to load
checkpoint = 'checkpoints/laboro_tomato_48ep.pth'
# initialize the detector
model = init_detector(config, checkpoint, device='cuda:0')

# Use the detector to do inference
img = 'laboro_tomato/train/IMG_0984.jpg'
result = inference_detector(model, img)

# Let's plot the result
show_result_pyplot(model, img, result, score_thr=0.3)


IMG_1003.jpg

laboro_tomato_48ep.pth


configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py

tomato/laboro_tomato_48ep.pth

tomato/IMG_1003.jpg

mmdetection/mmdetection/tomato/LaboroTomato/examples/ann_IMG_1066.png


bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
            
            
            ./tools/dist_test.sh configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py tomato/laboro_tomato_48ep.pth 4 --out results.pkl --eval bbox seg

Code: 
	./tools/dist_test.sh configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py tomato/laboro_tomato_48ep.pth 2 --out results.pkl --eval bbox
Result:
	loading annotations into memory...
	loading annotations into memory...
	Done (t=0.10s)
	creating index...
	index created!
	Done (t=0.10s)
	creating index...
	index created!
	load checkpoint from local path: tomato/laboro_tomato_48ep.pth
	load checkpoint from local path: tomato/laboro_tomato_48ep.pth
	[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 162/161, 1.1 task/s, elapsed: 146s, ETA:     0s
	writing results to results.pkl

	Evaluating bbox...
	Loading and preparing results...
	DONE (t=0.00s)
	creating index...
	index created!
	Running per image evaluation...
	Evaluate annotation type *bbox*
	DONE (t=1.15s).
	Accumulating evaluation results...
	DONE (t=0.11s).

	 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647
	 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.822
	 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.735
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.146
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.681
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.186
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.780

	OrderedDict([('bbox_mAP', 0.647), ('bbox_mAP_50', 0.822), ('bbox_mAP_75', 0.735), ('bbox_mAP_s', 0.0), ('bbox_mAP_m', 0.146), ('bbox_mAP_l', 0.681), ('bbox_mAP_copypaste', '0.647 0.822 0.735 0.000 0.146 0.681')])


Code: 
	./tools/dist_test.sh configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py tomato/laboro_tomato_48ep.pth 2 --out results.pkl --eval bbox segm

Result:
	Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647
	 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.822
	 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.735
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.146
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.681
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.742
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.186
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.780


	Evaluating segm...
	/mmdetection/mmdetection/mmdet/datasets/coco.py:456: UserWarning: The key "bbox" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.
	  UserWarning)
	Loading and preparing results...
	DONE (t=0.05s)
	creating index...
	index created!
	Running per image evaluation...
	Evaluate annotation type *segm*
	DONE (t=1.37s).
	Accumulating evaluation results...
	DONE (t=0.11s).

	 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660
	 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.818
	 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.736
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.131
	 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.697
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.751
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.751
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.751
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.187
	 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.791

	OrderedDict([('bbox_mAP', 0.647), ('bbox_mAP_50', 0.822), ('bbox_mAP_75', 0.735), ('bbox_mAP_s', 0.0), ('bbox_mAP_m', 0.146), ('bbox_mAP_l', 0.681), ('bbox_mAP_copypaste', '0.647 0.822 0.735 0.000 0.146 0.681'), ('segm_mAP', 0.66), ('segm_mAP_50', 0.818), ('segm_mAP_75', 0.736), ('segm_mAP_s', 0.0), ('segm_mAP_m', 0.131), ('segm_mAP_l', 0.697), ('segm_mAP_copypaste', '0.660 0.818 0.736 0.000 0.131 0.697')])



Show result:

mmdetection/mmdet/apis/inference.py 



